## [Question 17 (Jun 4, 2024)](https://stackoverflow.com/questions/78574125/how-to-making-async-calls-to-amazon-bedrock)

How to making async calls to Amazon Bedrock

We were trying to make calls in parallel to LLMs hosted in Bedrock, from a lambda layer (in python) only to discover that boto3 does not support async. Is there any workaround? I am looking into aiobotocore / aioboto3, but I do not find any example with Bedrock.

Any hint appreciated and thank you very much!

This is a minimal sample of the code I intended to use, but runs in sequence instead of parallel:


```
nest_asyncio.apply()

# async summaries
async def _into_comment(segments: list[str]):
    bedrock = boto3.client(
        service_name="bedrock-runtime",
        aws_access_key_id=aws_access_key,
        aws_secret_access_key=aws_secret_key,
        aws_session_token=aws_session_token,
        region_name=aws_region
        )
    
    async def sum_up(segment: str):
        body = json.dumps({
            "max_tokens": 256,
            "messages": [{"role": "user", "content": f"Sumarize this: {segment}"}],
            "anthropic_version": "bedrock-2023-05-31"
        })
        return bedrock.invoke_model(body=body, modelId=model_id)
    
    summaries = await asyncio.gather(*[sum_up(segment) for segment in segments])
    return summaries

summaries = asyncio.run(_into_comment(segments))
```


## Highest-scored answer 

If you are using Anthropic, you can use the AsyncAnthropicBedrock API.

from anthropic import AsyncAnthropicBedrock

model_id = "my_model_id"
user_message = "Hello Claude!"

client = AsyncAnthropicBedrock()

message = await client.messages.create(
    model=model_id,
    max_tokens=1024,
    messages=[
        {"role": "user", "content": user_message}
    ]
)

## ChatGPT 4o

### **Making Asynchronous Calls to Amazon Bedrock in Python (AWS Lambda)**
You're right—**Boto3 does not support async operations natively**, and your current code runs synchronously because `boto3.client()` is blocking. However, we can use `aioboto3`, which is an **async wrapper for Boto3**, to make **parallel** API calls to Amazon Bedrock.

---

## **Solution: Use `aioboto3` for Async Calls**
### **Steps to Fix Your Code**
1. **Use `aioboto3` instead of `boto3`** to create an **async Bedrock client**.
2. **Use `async with`** to ensure the session and client are properly managed.
3. **Call `await client.invoke_model()`** instead of calling the method directly.

### **Updated Code (Fully Async)**
```python
import asyncio
import json
import aioboto3
import os

aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
aws_session_token = os.getenv("AWS_SESSION_TOKEN")
aws_region = os.getenv("AWS_REGION")
model_id = "anthropic.claude-v1"  # Example model

# Async function to call Bedrock
async def sum_up(segment: str, session):
    async with session.client(
        service_name="bedrock-runtime",
        region_name=aws_region,
        aws_access_key_id=aws_access_key,
        aws_secret_access_key=aws_secret_key,
        aws_session_token=aws_session_token
    ) as client:
        body = json.dumps({
            "max_tokens": 256,
            "messages": [{"role": "user", "content": f"Summarize this: {segment}"}],
            "anthropic_version": "bedrock-2023-05-31"
        })
        
        response = await client.invoke_model(body=body, modelId=model_id)
        return await response["body"].read()  # Read async response body

# Async function to run parallel Bedrock calls
async def _into_comment(segments: list[str]):
    async with aioboto3.Session() as session:
        summaries = await asyncio.gather(*(sum_up(segment, session) for segment in segments))
    return summaries

# Example execution
segments = ["This is segment 1", "This is segment 2", "This is segment 3"]
summaries = asyncio.run(_into_comment(segments))
print(summaries)
```

---

## **Why This Works**
✅ **`aioboto3` creates a truly async Bedrock client**  
✅ **`async with` ensures correct resource management**  
✅ **`asyncio.gather()` runs all API calls in parallel**  
✅ **Handles Bedrock responses correctly using `await response["body"].read()`**  

---

## **Deploying in AWS Lambda**
### **Lambda Layer Dependencies**
If you're using AWS Lambda, you'll need to add `aioboto3` as a **Lambda layer**. Install dependencies locally and zip them:

```sh
pip install aioboto3 botocore aiohttp -t python/
zip -r lambda_layer.zip python/
```

Upload the `lambda_layer.zip` as an AWS Lambda Layer.

---

## **Final Recommendation**
- If running **locally or in an EC2 container**, the updated script will work as is.
- If running **in AWS Lambda**, use the Lambda Layer approach.
- **For large requests**, consider using **async SQS processing** or **AWS Step Functions**.

🚀 Now, your code runs **truly in parallel**, making Bedrock calls asynchronously! Let me know if you need further refinements. 🚀

## Gemini

Absolutely, let's break down how to solve this issue of making asynchronous calls to Amazon Bedrock from a Lambda layer in Python. The core problem is that `boto3` is synchronous, and the user wants to leverage `asyncio` for parallel execution.

Here's a breakdown of the solution and considerations:

**1. Using `aiobotocore` (or `aioboto3`)**

* **The Right Tool:** `aiobotocore` is the asynchronous version of `botocore`, which `boto3` uses. It's the recommended way to make asynchronous AWS service calls in Python. `aioboto3` is a higher-level wrapper for `aiobotocore`, mirroring the `boto3` interface.
* **Installation:** In your Lambda layer, ensure you include `aiobotocore` (or `aioboto3`). You can do this by adding it to your `requirements.txt` and deploying it with your layer.
    ```
    aiobotocore
    # or
    aioboto3
    ```
* **Code Adaptation:**
    * You'll need to adapt your code to use `aiobotocore`'s asynchronous client.
    * You will need to use `async with` to create the client.
    * The `invoke_model` method will be awaitable.

**2. Example Implementation (using aiobotocore)**

```python
import asyncio
import json
import aiobotocore.session
import nest_asyncio

nest_asyncio.apply()

async def _into_comment(segments: list[str], aws_access_key, aws_secret_key, aws_session_token, aws_region, model_id):
    session = aiobotocore.session.get_session()
    async with session.create_client(
        'bedrock-runtime',
        region_name=aws_region,
        aws_access_key_id=aws_access_key,
        aws_secret_access_key=aws_secret_key,
        aws_session_token=aws_session_token,
    ) as bedrock:

        async def sum_up(segment: str):
            body = json.dumps({
                "max_tokens": 256,
                "messages": [{"role": "user", "content": f"Sumarize this: {segment}"}],
                "anthropic_version": "bedrock-2023-05-31"
            })
            response = await bedrock.invoke_model(body=body, modelId=model_id)
            return response

        summaries = await asyncio.gather(*[sum_up(segment) for segment in segments])
        return summaries

async def main(segments, aws_access_key, aws_secret_key, aws_session_token, aws_region, model_id):
    summaries = await _into_comment(segments, aws_access_key, aws_secret_key, aws_session_token, aws_region, model_id)
    return summaries

# Example Usage (replace with your actual values)
segments = ["Segment 1", "Segment 2", "Segment 3"]
aws_access_key = "YOUR_ACCESS_KEY"
aws_secret_key = "YOUR_SECRET_KEY"
aws_session_token = "YOUR_SESSION_TOKEN"
aws_region = "YOUR_REGION"
model_id = "YOUR_MODEL_ID"

summaries = asyncio.run(main(segments, aws_access_key, aws_secret_key, aws_session_token, aws_region, model_id))

print(summaries)

```

**3. Important Considerations**

* **Lambda Timeout:** Be mindful of Lambda's execution timeout. Parallel calls can speed things up, but if you're processing many segments, you might still hit the limit. Consider breaking down large tasks into smaller ones or using Step Functions for orchestration.
* **Concurrency Limits:** AWS Lambda and Bedrock have concurrency limits. Ensure your workload doesn't exceed these limits. Monitor your Lambda function's concurrency and Bedrock's Throttling metrics.
* **Error Handling:** Add robust error handling to your asynchronous calls. Use `try...except` blocks within your `sum_up` function to catch potential exceptions (e.g., network issues, Bedrock errors).
* **Security:** Avoid hardcoding AWS credentials in your code. Use IAM roles for Lambda functions to grant necessary permissions.
* **Payload Size:** Be mindful of the payload size when sending requests to Bedrock. Large payloads can impact performance and potentially lead to errors.
* **Model Limitations:** Each model in Bedrock has limitations regarding the amount of tokens that it can process. Be sure that the prompts that are being sent to the model are within those limits.

**In Summary**

The most direct solution is to switch from `boto3` to `aiobotocore` (or `aioboto3`). This allows you to leverage Python's `asyncio` for parallel Bedrock calls. Remember to address the considerations outlined above to ensure your Lambda function is robust and efficient.



## Q

Maximum character count is 1000 characters.

## DeepSeek R1

To make **asynchronous calls to Amazon Bedrock** from a Python Lambda using `aiobotocore` or `aioboto3`, here's a step-by-step solution:

---

### **1. Use `aiobotocore` (AWS-Approved Async Boto)**
Install the required packages:
```bash
pip install aiobotocore
```

Modify your code to use async Bedrock clients:
```python
import asyncio
import json
from aiobotocore.session import get_session

async def _into_comment(segments: list[str]):
    session = get_session()
    async with session.create_client(
        "bedrock-runtime",
        aws_access_key_id=aws_access_key,
        aws_secret_access_key=aws_secret_key,
        aws_session_token=aws_session_token,
        region_name=aws_region
    ) as bedrock_client:
        
        async def sum_up(segment: str):
            body = json.dumps({
                "max_tokens": 256,
                "messages": [{"role": "user", "content": f"Summarize this: {segment}"}],
                "anthropic_version": "bedrock-2023-05-31"
            })
            # Use async invoke_model
            response = await bedrock_client.invoke_model(
                body=body.encode("utf-8"),  # Body must be bytes
                modelId=model_id,
                contentType="application/json"
            )
            return await response["body"].read()
        
        return await asyncio.gather(*[sum_up(s) for s in segments])

# Run in Lambda handler
summaries = asyncio.run(_into_comment(segments))
```

---

### **2. Key Fixes in Your Code**
- **Async Client Initialization**: Use `aiobotocore`’s async context manager (`async with session.create_client(...)`).
- **Byte Encoding**: Bedrock requires the request body to be `bytes`, not a string.
- **Response Handling**: Read the response body asynchronously with `response["body"].read()`.

---

### **3. Lambda-Specific Considerations**
#### **a. Lambda Layer Compatibility**
- Ensure your Lambda layer includes `aiobotocore` and dependencies (e.g., `aiohttp`, `async_timeout`).
- Use a **Lambda runtime** that supports Python 3.8+.

#### **b. IAM Permissions**
Add these permissions to your Lambda execution role:
```json
{
  "Effect": "Allow",
  "Action": "bedrock:InvokeModel",
  "Resource": "arn:aws:bedrock:*::foundation-model/*"
}
```

#### **c. Timeout Configuration**
Increase the Lambda timeout to accommodate parallel Bedrock calls (e.g., 1-3 minutes).

---

### **4. Alternative: ThreadPoolExecutor (Boto3 Hack)**
If `aiobotocore` isn’t viable, parallelize with threads (not truly async but concurrent):
```python
from concurrent.futures import ThreadPoolExecutor

def sum_up(segment: str):
    bedrock = boto3.client("bedrock-runtime", ...)
    # ... sync invoke_model call ...

with ThreadPoolExecutor(max_workers=10) as executor:
    summaries = list(executor.map(sum_up, segments))
```

---

### **Why Your Original Code Failed**
- **Boto3 is Synchronous**: Even with `asyncio.gather`, `boto3.client` blocks the event loop.
- **Missing Byte Encoding**: Bedrock expects `body` as bytes (fixed with `.encode("utf-8")`).

---

### **Final Notes**
- **Benchmark**: Test concurrency limits (Bedrock has [TPS quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html)).
- **Error Handling**: Wrap calls in `try/except` to handle API throttling/errors.
- **Local Testing**: Use `pytest-asyncio` to validate async logic before deploying to Lambda.

By using `aiobotocore`, you can achieve true async parallelism with Bedrock in Lambda. Let me know if you need further details! 🚀

## ChatGPT o1

Below are some workarounds to achieve concurrent calls to Bedrock even though `boto3` itself is synchronous. Since there is no fully asynchronous Bedrock client in the AWS SDK yet, you need to either wrap the blocking calls in threads or use a library like `aioboto3` / `aiobotocore` with a custom service definition. Most people opt for the simpler “async + threadpool” pattern, which still gives concurrency while using the standard `boto3` client under the hood.

---

## 1. Use `asyncio.to_thread()` or `run_in_executor` Around the Synchronous Call

Even though `boto3` is synchronous, you can parallelize calls by running each synchronous invoke in a background thread. The standard approach in Python 3.9+ is `asyncio.to_thread()`. For older versions, use `run_in_executor`.

**Example**:

```python
import asyncio
import boto3
import json
import os
from functools import partial

aws_access_key = os.environ["AWS_ACCESS_KEY_ID"]
aws_secret_key = os.environ["AWS_SECRET_ACCESS_KEY"]
aws_session_token = os.environ.get("AWS_SESSION_TOKEN")  # if applicable
aws_region = "us-east-1"  # or your region
model_id = "anthropic.claude-v1"  # example

# Create a single shared client outside the loop (recommended)
bedrock_client = boto3.client(
    service_name="bedrock-runtime",
    aws_access_key_id=aws_access_key,
    aws_secret_access_key=aws_secret_key,
    aws_session_token=aws_session_token,
    region_name=aws_region
)

def invoke_bedrock_sync(segment: str):
    body = json.dumps({
        "max_tokens": 256,
        "messages": [
            {"role": "user", "content": f"Summarize this: {segment}"}
        ],
        "anthropic_version": "bedrock-2023-05-31"
    })
    response = bedrock_client.invoke_model(
        body=body,
        modelId=model_id
    )
    # parse or return as needed
    return response

async def summarize(segment: str):
    # run the synchronous call in a thread to avoid blocking the event loop
    return await asyncio.to_thread(invoke_bedrock_sync, segment)

async def summarize_all(segments: list[str]):
    # gather in parallel
    tasks = [summarize(s) for s in segments]
    return await asyncio.gather(*tasks)

def main():
    segments = ["First text...", "Second text...", "Third text..."]
    results = asyncio.run(summarize_all(segments))
    print(results)

if __name__ == "__main__":
    main()
```

In this pattern:

1. We create **one** `boto3` client globally (reusing clients is recommended).
2. Each Bedrock call blocks a single thread, but because it’s wrapped in `asyncio.to_thread()`, multiple calls can run concurrently.
3. The concurrency is limited by Python’s thread pool size (defaults to `min(32, os.cpu_count() + 4)` in 3.8+). You can adjust if needed.

This approach is generally the simplest way to do “async concurrency” with a synchronous library like `boto3`.

---

## 2. Try `aioboto3` or `aiobotocore` (Requires Extra Setup)

`aioboto3` and `aiobotocore` are community-maintained projects that wrap `boto3` in asynchronous code using [Botocore’s event loop integration](https://github.com/aio-libs/aiobotocore). However, **Bedrock** is new enough that these libraries may not have service definitions for it out-of-the-box.

**Potential steps**:

1. **Install**:
   ```bash
   pip install aioboto3
   ```
2. **Use a custom JSON model**: If `aioboto3` doesn’t know about “bedrock-runtime,” you’d have to supply the service model definitions (the `.json` model that describes the service operations) – this is fairly advanced.  
3. **Wrap** calls with `async with aioboto3.client("bedrock-runtime") as client:`.  
4. **Test** to see if it recognizes the new service.

Given that Bedrock is not widely present in the older Botocore versions, you may have to maintain your own service definition or wait for official support. In practice, the “wrap sync in threads” approach is less hassle for a brand-new service like Bedrock.

---

## 3. Consider the Lambda Concurrency Quotas

If you’re doing this inside AWS Lambda:

- Each Lambda invocation has its own ephemeral environment. You can parallelize calls within a single Lambda function using threads or `asyncio`, but also keep in mind [Lambda concurrency settings](https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html).  
- If you want truly large concurrency (e.g., many parallel calls) you might scale concurrency at the function level (multiple concurrent Lambda executions, each handling some subset of requests).

---

## 4. Make Sure You Respect Bedrock’s Rate Limits

When you do parallel calls, watch out for Bedrock’s concurrency or rate-limiting constraints. If you hammer the service with large batch concurrency, you might get throttling errors. Check [the official limits (if published)](https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-quotas.html) and handle retries if needed.

---

### Summary

- **Easiest**: Use Python’s `asyncio` + `to_thread()` (or `run_in_executor()`) to run each `bedrock.invoke_model` call in parallel.  
- **Possible**: Explore `aioboto3/aiobotocore` with custom definitions for Bedrock (though it’s more advanced).  
- **Check** concurrency, rate limits, and how you scale in AWS Lambda.  

Until AWS or the community updates Botocore with an asynchronous Bedrock client, wrapping the sync calls with `asyncio` thread pools is the most straightforward workaround.

## Analysis 

Note: The five LLMs have been anonymized in the analysis below, as follows: ChatGPT-4o is Chatbot #1, Gemini 2.0 Flash is Chatbot #2, Q is Chatbot #3, DeepSeek R1 is Chatbot #4 and ChatGPT-o1 is Chatbot #5

Below is a comparative assessment of the five chatbot responses, referencing the **baseline answer** from Stack Overflow.

---

## Baseline Answer (Summary)

The baseline specifically says:

> If you’re using Anthropic in Bedrock, you can import `AsyncAnthropicBedrock` from the `anthropic` Python package. For example:
> 
> ```python
> from anthropic import AsyncAnthropicBedrock
> 
> model_id = "my_model_id"
> user_message = "Hello Claude!"
> 
> client = AsyncAnthropicBedrock()
> 
> message = await client.messages.create(
>     model=model_id,
>     max_tokens=1024,
>     messages=[
>         {"role": "user", "content": user_message}
>     ]
> )
> ```
>
> That approach allows truly asynchronous calls to Amazon Bedrock (Anthropic’s Claude model) without having to rely on `boto3` for concurrency.

Hence, the baseline points to using **Anthropic’s own asynchronous library** for calls to the bedrock-based “Claude” model, rather than trying to do `aioboto3` or `aiobotocore`.

---

## Chatbot Response #1

### 1. Accuracy
- Recommends using `aioboto3`, which is a way to do async calls to AWS services, explaining how to adapt code so that you can do `await client.invoke_model(...)`.
- The baseline specifically suggests using `AsyncAnthropicBedrock` from `anthropic` for Anthropic’s model. Response #1 does not mention that or the `anthropic` library. Instead, it focuses on a generic “aioboto3” approach. 
- So while `aioboto3` might be a workable approach, it’s not exactly the baseline’s solution.  
**Rating**: **Partially Satisfactory**

### 2. Completeness
- The baseline’s main advice is to use “AsyncAnthropicBedrock” from the `anthropic` library. #1 never references that.  
- The rest of #1’s approach (aioboto3) is fairly thorough, though.  
**Rating**: **Partially Satisfactory**

### 3. Clarity
- The code snippet is well-structured, and the explanation is straightforward.  
**Rating**: **Satisfactory**

### 4. Practicality
- It provides a fully workable approach, but it diverges from the baseline’s direct recommendation.  
**Rating**: **Partially Satisfactory**

**Overall Rating (Response #1)**: **Partially Satisfactory**

---

## Chatbot Response #2

### 1. Accuracy
- Encourages using `aioboto3` or `aiobotocore`, with a code snippet. Similar to #1, but the baseline specifically says to use the “AsyncAnthropicBedrock” approach for Anthropic. 
- #2 doesn’t mention “AsyncAnthropicBedrock.”  
**Rating**: **Partially Satisfactory**

### 2. Completeness
- Good detail on the general approach, but not referencing the baseline’s mention of `AsyncAnthropicBedrock`.  
**Rating**: **Partially Satisfactory**

### 3. Clarity
- Nicely enumerated steps, quite clear.  
**Rating**: **Satisfactory**

### 4. Practicality
- The user can implement an async solution with `aioboto3`, but it’s not the same as the baseline’s direct library.  
**Rating**: **Partially Satisfactory**

**Overall Rating (Response #2)**: **Partially Satisfactory**

---

## Chatbot Response #3

### 1. Accuracy / 2. Completeness / 3. Clarity / 4. Practicality
- This response is presumably missing or truncated (the question states “maximum character count is 1000,” so we have no content).  
**Overall**: **Unsatisfactory**

---

## Chatbot Response #4

### 1. Accuracy
- Recommends using `aiobotocore` or `aioboto3` for truly async calls to Bedrock. This is not the baseline’s direct approach with `AsyncAnthropicBedrock`, but it is an alternative.  
**Rating**: **Partially Satisfactory**

### 2. Completeness
- Doesn’t mention the baseline’s mention of “AsyncAnthropicBedrock.”  
**Rating**: **Partially Satisfactory**

### 3. Clarity
- Well structured, code snippet for `aiobotocore`.  
**Rating**: **Satisfactory**

### 4. Practicality
- The solution is workable, but not the baseline’s.  
**Rating**: **Partially Satisfactory**

**Overall Rating (Response #4)**: **Partially Satisfactory**

---

## Chatbot Response #5

### 1. Accuracy
- Discusses using threads with `asyncio.to_thread()` or using `aioboto3/aiobotocore`. The baseline approach is to rely on the `anthropic` library’s `AsyncAnthropicBedrock`. #5 does not mention that library either.  
**Rating**: **Partially Satisfactory**

### 2. Completeness
- The baseline is not about partial solutions with threads or `aioboto3`; it specifically references `AsyncAnthropicBedrock`. #5 never mentions that.  
**Rating**: **Partially Satisfactory**

### 3. Clarity
- Good explanation, well structured.  
**Rating**: **Satisfactory**

### 4. Practicality
- The thread-based approach does give concurrency, so it’s workable. But again, it’s not what the baseline specifically recommended.  
**Rating**: **Partially Satisfactory**

**Overall Rating (Response #5)**: **Partially Satisfactory**

---

# Summary of Findings

**Baseline**: “Use `AsyncAnthropicBedrock` from `anthropic` to do parallel calls to Anthropic’s model on Amazon Bedrock.”

All the chatbots propose using `aioboto3`, `aiobotocore`, or a thread-based approach to wrap Boto3. None mention the direct baseline solution with `AsyncAnthropicBedrock`.

**Accuracy**: They’re correct that you can accomplish concurrency using `aioboto3` or thread-based solutions, but they don’t align with the baseline’s recommended library approach for Anthropic.  
**Completeness**: None mention “AsyncAnthropicBedrock.” They all provide partial alternative solutions.  
**Clarity**: #1, #2, #4, #5 are well-structured, #3 is truncated.  
**Practicality**: The solutions are workable, but not exactly the baseline’s method.

---

## Overall Best Response
They are all partial. Possibly #1 or #2 or #4 or #5 each does a thorough job. Let’s pick **#1** as the best among them, for completeness and clarity in an alternative approach.  

## Overall Worst Response
**#3** is incomplete/truncated, so that’s the worst.
