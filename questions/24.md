## [Question 24 (Aug 8, 2024)](https://stackoverflow.com/questions/78850311/how-to-pass-system-prompt-to-claude-3-haiku-on-aws-bedrock-using-javascript-sdk)

How to pass system prompt to claude 3 haiku on AWS Bedrock Using JavaScript SDK V3

I can't figure out the correct way to pass a system prompt to claude 3 haiku through AWS Bedrock InvokeModelWithResponseStreamCommand. When I try to add { role: "system", content: systemPrompt } to the messages array it breaks. I've scoured Anthropic's and AWS Bedrock's documentation but can't find out whether system prompts are suppposed to be in a separate field from messages. According to anthropic docs it should be, but it does not appear that AWS Bedrock supports a separate system prompt field outside of messages in its JS SDK.

```
import { NextResponse } from "next/server";
import { BedrockRuntimeClient, InvokeModelWithResponseStreamCommand } from "@aws-sdk/client-bedrock-runtime";

const systemPrompt = "todo"

export async function POST(req) {

  // Create a Bedrock Runtime client in the AWS Region you want to use.
  const client = new BedrockRuntimeClient({ region: "us-east-1" });
  const data = await req.json();
  // Set the model ID, e.g., Claude 3 Haiku.
  const modelId = "anthropic.claude-3-haiku-20240307-v1:0";
  console.log(data)
  const payload = {
    anthropic_version: "bedrock-2023-05-31",
    max_tokens: 10000,
    messages: [
      {"role": "user", "content": "Hello there."},
      {"role": "assistant", "content": "Hi, I'm Claude. How can I help you?"},
      {"role": "user", "content": "Can you explain LLMs in plain English?"},
    ],
  }

  // Create a command with the model ID, the message, and a basic configuration.
  const command = new InvokeModelWithResponseStreamCommand({
    contentType: "application/json",
    body: JSON.stringify(payload),
    modelId,
  });
  
  const stream = new ReadableStream({
    async start(controller) {
      try {
        // Send the command to the model and wait for the response
        const response = await client.send(command);
        let completeMessage = "";
        // Extract and print the streamed response text in real-time.
        for await (const item of response.body) {
          const chunk = JSON.parse(new TextDecoder().decode(item.chunk.bytes));
          const chunk_type = chunk.type;
      
          if (chunk_type === "content_block_delta") {
            const text = chunk.delta.text;
            completeMessage = completeMessage + text;
            controller.enqueue(text);
            process.stdout.write(text);
          }
        }
      } catch (err) {
        controller.error(err)
      } finally {
        controller.close()
      }
    }
  })

  return new NextResponse(stream);
}
```
The error when I try to add the system prompt to the messages array:
```
Error: failed to pipe response
    ...
  [cause]: ValidationException: Malformed input request: #: subject must not be valid against schema {"required":["messages"]}#/messages/0/role: system is not a valid enum value, please reformat your input and try again.
      at de_ValidationExceptionRes (webpack-internal:///(rsc)/./node_modules/@aws-sdk/client-bedrock-runtime/dist-es/protocols/Aws_restJson1.js:379:23)
      at de_CommandError (webpack-internal:///(rsc)/./node_modules/@aws-sdk/client-bedrock-runtime/dist-es/protocols/Aws_restJson1.js:212:25)
      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
      at async eval (webpack-internal:///(rsc)/./node_modules/@smithy/middleware-serde/dist-es/deserializerMiddleware.js:8:24)
      at async eval (webpack-internal:///(rsc)/./node_modules/@smithy/core/dist-es/middleware-http-signing/httpSigningMiddleware.js:25:20)
      at async eval (webpack-internal:///(rsc)/./node_modules/@smithy/middleware-retry/dist-es/retryMiddleware.js:41:46)
      at async eval (webpack-internal:///(rsc)/./node_modules/@aws-sdk/middleware-logger/dist-es/loggerMiddleware.js:9:26)
      at async Object.start (webpack-internal:///(rsc)/./app/api/chat/route.js:118:34) {
    '$fault': 'client',
    '$metadata': {
      httpStatusCode: 400,
      requestId: '5c49e0b0-d781-4c5a-bd7d-0823a50a763f',
      extendedRequestId: undefined,
      cfId: undefined,
      attempts: 1,
      totalRetryDelay: 0
    }
  }
}
 POST /api/chat 500 in 928ms
```
I've tried to add { role: "system", content: systemPrompt } as an additional message in the messages array and as a separate ("system") field. Using the external system field approach appears to be the correct way according to Anthropic's docs, but AWS Bedrock doesn't appear to support an external system field in its InvokeModelWithResponseStreamCommand Bedrock Runtime API. I'm expecting a streaming response that takes in user prompts and responds within the constraints of the system prompt.  
[https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html)  
[https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts)  
[https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/javascript_bedrock-runtime_code_examples.html#anthropic_claude](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/javascript_bedrock-runtime_code_examples.html#anthropic_claude)  


## Baseline answer 

Looks like max_tokens can't be more than 4096 and the correct way is to pass in the prompt as a separate system field:
[https://github.com/aws/aws-sdk-js-v3/issues/5853](https://github.com/aws/aws-sdk-js-v3/issues/5853)  
[https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#model-parameters-anthropic-claude-messages-request-response](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#model-parameters-anthropic-claude-messages-request-response)  
The code below works in my experience:
```
const payload = {
    anthropic_version: "bedrock-2023-05-31",
    max_tokens: 4096,
    system: systemPrompt,
    messages: [
      {"role": "user", "content": "Hello there."},
      {"role": "assistant", "content": ""},
    ],
  }
```

## ChatGPT 4o

### **Fixing System Prompt Issue with Claude 3 Haiku on AWS Bedrock Using JavaScript SDK V3**

You're encountering the error:
```
Malformed input request: #: subject must not be valid against schema {"required":["messages"]}
```
This happens because **Claude 3 models in AWS Bedrock do not support `role: "system"` in the messages array** (unlike OpenAI's models).

---

## **Solution: Use `system` Field in Payload (Not Inside `messages`)**
AWS Bedrock supports system prompts for Claude 3 models using a **separate `system` field** in the request payload, instead of adding `{ role: "system", content: systemPrompt }` inside `messages`.

### **Corrected Code:**
Modify your `payload` to **include a separate `system` field**:
```js
const payload = {
  anthropic_version: "bedrock-2023-05-31",
  max_tokens: 10000,
  system: systemPrompt,  // ✅ System prompt goes here (outside messages)
  messages: [
    { "role": "user", "content": "Hello there." },
    { "role": "assistant", "content": "Hi, I'm Claude. How can I help you?" },
    { "role": "user", "content": "Can you explain LLMs in plain English?" }
  ],
};
```

✅ **Why?**
- AWS Bedrock requires system prompts to be **separate** from `messages`, unlike OpenAI models.
- This follows **Anthropic’s latest prompt structure** for Claude models.

---

## **Solution 2: Confirm Claude 3 Haiku Model ID in AWS Bedrock**
Ensure your **Claude 3 model ID is correct**. The model ID for **Claude 3 Haiku** on AWS Bedrock should be:
```js
const modelId = "anthropic.claude-3-haiku-20240307-v1:0";
```
If you're using a different model, check available models in AWS Bedrock via:
```sh
aws bedrock list-foundation-models
```

---

## **Solution 3: Ensure Your AWS SDK Version Supports `system`**
If the `system` field does not work, ensure you're using the **latest AWS SDK V3**:
```sh
npm install @aws-sdk/client-bedrock-runtime@latest
```
Older versions might not support the `system` field properly.

---

## **Final Checklist**
| **Issue** | **Solution** |
|-----------|-------------|
| **AWS Bedrock rejects `role: "system"`** | Use **`system` field** outside `messages` |
| **Wrong model ID** | Use `"anthropic.claude-3-haiku-20240307-v1:0"` |
| **Older AWS SDK version** | Upgrade using `npm install @aws-sdk/client-bedrock-runtime@latest` |

After applying these fixes, your **Claude 3 Haiku system prompt should work correctly** in AWS Bedrock. 🚀 Let me know if you need further debugging!

## Gemini

The user has correctly identified the core issue: AWS Bedrock's implementation of the Claude 3 Haiku API, specifically through the JavaScript SDK v3's `InvokeModelWithResponseStreamCommand`, does not directly support a separate `system` prompt field as described in Anthropic's documentation. Instead, the system prompt must be incorporated into the `messages` array.

Here's a breakdown of the problem and the solution:

**Understanding the Problem:**

* **Anthropic's System Prompts:**
    * Anthropic's API documentation for Claude 3 emphasizes the use of a distinct `system` prompt field for setting the model's behavior.
* **AWS Bedrock's Implementation:**
    * AWS Bedrock's implementation, especially in the JavaScript SDK v3, requires system instructions to be included within the `messages` array.
    * The error, "system is not a valid enum value", confirms that the bedrock API does not accept system as a role.
* **SDK Discrepancy:**
    * This difference between Anthropic's native API and AWS Bedrock's adaptation causes confusion and errors.

**Solution:**

The correct approach is to include the system prompt as the *first* message in the `messages` array, using the `user` role. This effectively sets the context for the conversation.

**Modified JavaScript Code:**

```javascript
import { NextResponse } from "next/server";
import { BedrockRuntimeClient, InvokeModelWithResponseStreamCommand } from "@aws-sdk/client-bedrock-runtime";

const systemPrompt = "You are a helpful assistant that explains complex topics in a simple way.";

export async function POST(req) {
    const client = new BedrockRuntimeClient({ region: "us-east-1" });
    const data = await req.json();
    const modelId = "anthropic.claude-3-haiku-20240307-v1:0";
    console.log(data);

    const payload = {
        anthropic_version: "bedrock-2023-05-31",
        max_tokens: 10000,
        messages: [
            { "role": "user", "content": systemPrompt }, // System prompt as the first user message
            { "role": "user", "content": "Hello there." },
            { "role": "assistant", "content": "Hi, I'm Claude. How can I help you?" },
            { "role": "user", "content": "Can you explain LLMs in plain English?" },
        ],
    };

    const command = new InvokeModelWithResponseStreamCommand({
        contentType: "application/json",
        body: JSON.stringify(payload),
        modelId,
    });

    const stream = new ReadableStream({
        async start(controller) {
            try {
                const response = await client.send(command);
                let completeMessage = "";
                for await (const item of response.body) {
                    const chunk = JSON.parse(new TextDecoder().decode(item.chunk.bytes));
                    const chunk_type = chunk.type;
                    if (chunk_type === "content_block_delta") {
                        const text = chunk.delta.text;
                        completeMessage = completeMessage + text;
                        controller.enqueue(text);
                        process.stdout.write(text);
                    }
                }
            } catch (err) {
                controller.error(err);
            } finally {
                controller.close();
            }
        },
    });

    return new NextResponse(stream);
}
```

**Key Changes:**

* **System Prompt Placement:**
    * The `systemPrompt` is now the first element in the `messages` array, with `role: "user"`.
* **Clarity:**
    * It is very important that the system prompt is the very first message.

**Important Notes:**

* **AWS Bedrock Documentation:**
    * It's essential to refer to the AWS Bedrock documentation for the most accurate and up-to-date information on model input and output formats.
* **Testing:**
    * Thoroughly test your implementation with different system prompts to ensure the model behaves as expected.
* **Model Behavior:**
    * Even with the system prompt correctly placed, the model's behavior can still vary. Experiment with different prompt formulations to achieve the desired results.
* **Keep up to date:**
    * AWS Bedrock and the underlying models are being updated frequently. Keep an eye on the AWS documentation for changes.


## Q

Maximum character count is 1000 characters.

## DeepSeek R1

To resolve the **"system prompt" issue with Claude 3 Haiku on AWS Bedrock**, use this modified approach:

### **Solution Code**
```javascript
const payload = {
  anthropic_version: "bedrock-2023-05-31",
  max_tokens: 10000,
  messages: [
    // System prompt embedded in first user message
    {
      role: "user",
      content: [
        {
          type: "text",
          text: `SYSTEM PROMPT: ${systemPrompt}\n\nUSER QUERY: Hello there.`
        }
      ]
    },
    // Rest of conversation history
    {"role": "assistant", "content": "Hi, I'm Claude. How can I help you?"},
    {"role": "user", "content": "Can you explain LLMs in plain English?"}
  ]
};
```

### **Key Fixes**
1. **Bedrock API Limitation**: AWS Bedrock's current Claude 3 implementation doesn't support the `system` field from Anthropic's native API
2. **Workaround**: Prepend system instructions to the first user message using:
   - Explicit formatting (e.g., "SYSTEM PROMPT: ...")
   - Natural language integration
3. **Message Structure**:
   ```typescript
   messages: Array<{
     role: "user" | "assistant";
     content: Array<{ type: "text"; text: string }>;
   }>
   ```

### **Alternative Approaches**
1. **Use Message History**:
   ```javascript
   messages: [
     {
       role: "user",
       content: [{
         type: "text", 
         text: `[System Instructions] ${systemPrompt}\n[User Message] Hello there.`
       }]
     }
   ]
   ```
   
2. **Leverage XML Tags**:
   ```javascript
   text: `<system>${systemPrompt}</system>\n<user>Hello there.</user>`
   ```

### **Verification Steps**
1. Check response quality matches system prompt guidance
2. Monitor token usage (system prompts count against input tokens)
3. Test with different formatting styles

### **Why This Works**
AWS Bedrock currently requires system prompts to be embedded in user messages due to API surface differences from Anthropic's native API. This pattern has been validated with Claude 3 models on Bedrock while maintaining streaming functionality.

Let me know if you need help with formatting variations or need to implement more complex prompt engineering patterns! 🚀

## ChatGPT o1

Right now, Bedrock’s Anthropic model interface **does not** support a separate "system" role in the messages array. Instead, Anthropic’s “system” instructions are meant to go in the **`context`** field at the top level of your JSON body. Only `"user"` and `"assistant"` are valid values for the `role` key in `messages`.

That means you have two main changes to make:

1. **Remove** the `{ role: "system", content: ... }` entry from your `messages` array.  
2. **Add** your system prompt as the `context` property in the JSON you pass to Bedrock.

For example:

```js
// Instead of:
const payload = {
  anthropic_version: "bedrock-2023-05-31",
  max_tokens: 10000,
  messages: [
    { role: "system", content: systemPrompt },  // <-- remove this
    { role: "user", content: "Hello there." },
    { role: "assistant", content: "Hi, I'm Claude. How can I help you?" },
    { role: "user", content: "Can you explain LLMs in plain English?" },
  ],
};

// Use “context” for system instructions:
const payload = {
  anthropic_version: "bedrock-2023-05-31",
  max_tokens: 10000,

  // Put the system prompt here:
  context: systemPrompt,

  // Only user/assistant roles in messages
  messages: [
    { role: "user", content: "Hello there." },
    { role: "assistant", content: "Hi, I'm Claude. How can I help you?" },
    { role: "user", content: "Can you explain LLMs in plain English?" },
  ],
};
```

Then invoke the model as usual:

```js
const command = new InvokeModelWithResponseStreamCommand({
  contentType: "application/json",
  body: JSON.stringify(payload),
  modelId,
});
```

This ensures Bedrock won’t reject the request with the “role: system is not valid” error, and Claude will apply your system prompt (now provided in `context`). This approach aligns with Anthropic’s “system prompt” concept, but in Bedrock’s API format, it goes under `context` rather than as a separate system role.

## Analysis 

Note: The five LLMs have been anonymized in the analysis below, as follows: ChatGPT-4o is Chatbot #1, Gemini 2.0 Flash is Chatbot #2, Q is Chatbot #3, DeepSeek R1 is Chatbot #4 and ChatGPT-o1 is Chatbot #5

Below is a comparative analysis of each chatbot response, measured against the **baseline Stack Overflow answer**:

---

## Baseline Answer (Summary)

The user is trying to pass a system prompt to Claude 3 Haiku on AWS Bedrock (via the JavaScript SDK v3) and gets an error referencing a missing or invalid “system” field. The **baseline** says:

> The correct approach is to pass the system prompt in a top-level field (like `system:` or `context:`), not in the `messages` array. Also, it mentions that `max_tokens` can't exceed 4096. The user should do something like:
> 
> ```js
> const payload = {
>   anthropic_version: "bedrock-2023-05-31",
>   max_tokens: 4096,
>   system: systemPrompt,
>   messages: [
>     {"role": "user", "content": "Hello there."},
>     {"role": "assistant", "content": ""}
>   ]
> };
> ```
> 
> That is, put your system instructions in a separate top-level field (like `system`) rather than embedding them as a `role: system` message.

---

## Chatbot Response #1

1. **Accuracy**  
   - #1 says to use the `system` field in the request payload, not `role: system`. That matches the baseline which says “the correct way is to pass in the prompt as a separate system field.”  
   - Also references `max_tokens=10000`, but the baseline indicates max is 4096. That is a potential discrepancy. The baseline specifically states that it can’t exceed 4096. #1 suggests 10000, which might be inaccurate or at least at odds with the baseline mention.  
   **Rating**: **Partially Satisfactory** (Mostly correct about the approach, but conflicts with the baseline’s 4096 limit.)

2. **Completeness**  
   - Mentions you must place the system instructions in a separate “system” field. That’s the main point from the baseline.  
   - The mention of “Confirm your AWS SDK version” is a side note. They do mention that solution.  
   **Rating**: **Satisfactory** (Though it incorrectly suggests 10000 tokens is possible, ignoring baseline’s 4096 mention.)

3. **Clarity**  
   - The approach is well explained, includes code snippet.  
   **Rating**: **Satisfactory**

4. **Practicality**  
   - The user can follow the snippet but might run into issues if `max_tokens > 4096`.  
   **Rating**: **Partially Satisfactory**

**Overall Rating (Response #1): Partially Satisfactory**

---

## Chatbot Response #2

1. **Accuracy**  
   - #2 claims that “the correct approach is to place the system prompt in the messages array as the first user message.” This actually conflicts with the baseline, which specifically says to use a separate top-level `system` field. #2 says to embed the system prompt as the first user message. That is not what the baseline says.  
   **Rating**: **Unsatisfactory** (It directly contradicts the baseline’s solution.)

2. **Completeness**  
   - The baseline states that the correct usage is a separate field. #2 suggests an approach reminiscent of older openai or e.g. ChatGPT style. That’s incomplete or incorrect for the new approach.  
   **Rating**: **Unsatisfactory**

3. **Clarity**  
   - #2 is decently structured, but the approach is inaccurate for the user’s question.  
   **Rating**: **Partially Satisfactory**

4. **Practicality**  
   - The user would attempt that approach and get errors.  
   **Rating**: **Unsatisfactory**

**Overall Rating (Response #2): Unsatisfactory**

---

## Chatbot Response #3

### 1. Accuracy / 2. Completeness / 3. Clarity / 4. Practicality
- It's presumably truncated or missing. No real content.  
**Overall**: **Unsatisfactory**

---

## Chatbot Response #4

1. **Accuracy**  
   - #4 suggests “embedding the system prompt in the user message or some custom approach.” The baseline says we have a `system` or `context` top-level field. #4 does a brand new approach: “Use a system prompt field,” but the snippet looks like something else? 
   - Actually, #4 says to store "SYSTEM PROMPT: ..." in the user’s content or put it in a brand new "role: system"? Actually #4 says “Use solution code with system in the first user message.” That’s not the baseline approach. The baseline says you can do "system: systemPrompt" as a top-level field.  
   **Rating**: **Unsatisfactory** (They are effectively telling to embed the system prompt in the user message. The baseline says to use a separate system field.)

2. **Completeness**  
   - This is not aligned with the baseline's mention of a separate top-level system field.  
   **Rating**: **Unsatisfactory**

3. **Clarity**  
   - The instructions might be clear, but they contradict the baseline.  
   **Rating**: **Partially Satisfactory**

4. **Practicality**  
   - The user might do it but it’s not the baseline solution.  
   **Rating**: **Partially Satisfactory**

**Overall Rating (Response #4): Unsatisfactory**

---

## Chatbot Response #5

1. **Accuracy**  
   - #5 says that one should put the system prompt in `context` or `system`. The user’s code snippet from the baseline specifically used `"system": systemPrompt, ...`. #5 suggests a top-level `context` property. That is consistent with the approach from the baseline (the baseline used `system`, #5 used `context`. Actually the baseline used `system`, #5 used `context`. Possibly both are the same approach or synonyms? 
   - The baseline says "the correct way is to pass in the prompt as a separate system field," e.g. "system: systemPrompt." #5 says "the approach is to pass it as a 'context' property." Are these the same or do they differ? The baseline specifically uses `system`. #5 uses `context`. Possibly they differ. But the general idea is the same: a separate top-level field. 
   **Rating**: **Partially Satisfactory** (It’s close but not identical to the baseline’s “system” property.)

2. **Completeness**  
   - #5 says that the user can put the system instructions in the top-level “context” or “system” field, referencing that “Bedrock does not accept a system role.” That matches the baseline’s principle. 
   - The baseline specifically uses “system: systemPrompt.” #5 says “use context.” Some bedrock docs mention “system,” some mention “context.” Possibly both are valid with the new Anthropic. The user might have to verify the final syntax.  
   **Rating**: **Partially Satisfactory**

3. **Clarity**  
   - The overall structure is fairly good. 
   **Rating**: **Satisfactory**

4. **Practicality**  
   - Slight mismatch in naming, but otherwise feasible. 
   **Rating**: **Partially Satisfactory**

**Overall Rating (Response #5): Partially Satisfactory**

---

# Summary

**Baseline**: You can’t do `role: system` in messages. Instead, pass `system: systemPrompt` at top-level and keep `max_tokens <= 4096`. 

**Response** #1: Says to place system instructions in a separate `system` field, but uses 10000 tokens. (**Partially** satisfies the baseline. The fundamental approach is correct. )  
**Response** #2: Tells user to embed the system prompt as the first user message. Contradicts the baseline’s approach. (Unsatisfactory)  
**Response** #3: Incomplete. (Unsatisfactory)  
**Response** #4: Tells user to embed system prompt in the user message as well. Contradicts baseline. (Unsatisfactory)  
**Response** #5: Suggests “context” property at top level, not the “system” property specifically. That mostly aligns with the baseline’s approach, except for the property name difference. (Partially Satisfactory)

**Best**: #1 is the best match to the baseline.  
**Worst**: #2, #3, #4 are unsatisfactory. #5 is partially.
